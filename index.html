<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <!-- Meta tags for social media banners -->
  <meta name="description" content="USERASSIST: A benchmark for user–assistant bias in LLMs. We evaluate 52 models, analyze post-training causes, and show bias is tunable with lightweight DPO." />
  <meta property="og:title" content="User–Assistant Bias in LLMs" />
  <meta property="og:description" content="USERASSIST: A benchmark for user–assistant bias in LLMs. Results across 52 models; preference alignment ↑ user bias, reasoning traces ↓ it; bias is tunable via DPO." />
  <meta property="og:url" content="https://YOUR-DOMAIN.TLD/userassist" />
  <meta property="og:image" content="static/images/userassist_og.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />

  <meta name="twitter:title" content="User–Assistant Bias in LLMs" />
  <meta name="twitter:description" content="Measuring and controlling user–assistant bias in LLMs with USERASSIST. 52-model benchmark + controllability via lightweight DPO." />
  <meta name="twitter:image" content="static/images/userassist_twitter.png" />
  <meta name="twitter:card" content="summary_large_image" />

  <meta name="keywords" content="LLMs, user–assistant bias, sycophancy, stubbornness, multi-turn conversations, USERASSIST, DPO, alignment, reasoning traces, dataset, benchmark" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>User–Assistant Bias in LLMs</title>
  <link rel="icon" type="image/x-icon" href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQYV2NkYGD4DwABBAEAja6zVwAAAABJRU5ErkJggg==" />
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet" />

  <link rel="stylesheet" href="static/css/bulma.min.css" />
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css" />
  
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" />
  <link rel="stylesheet" href="static/css/index.css" />

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  
  <script src="static/js/index.js"></script>
</head>
<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">User–Assistant Bias in LLMs</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors; * indicates equal contribution -->
              <span class="author-block"><a href="https://scholar.google.com/citations?user=y-DixhMAAAAJ&hl=en" target="_blank">Xu Pan</a><sup>*1</sup>,</span>
              <span class="author-block"><a href="https://jingxuanf0214.github.io/" target="_blank">Jingxuan Fan</a><sup>*1</sup>,</span>
              <span class="author-block"><a href="https://polaris-73.github.io/" target="_blank">Zidi Xiong</a><sup>1</sup>,</span>
              <span class="author-block"><a href="https://scholar.google.com/citations?user=5fXhVkoAAAAJ&hl=en" target="_blank">Ely Hahami</a><sup>1</sup>,</span>
              <span class="author-block"><a href="https://scholar.google.com/citations?user=XDpDiuYAAAAJ&hl=en" target="_blank">Jorin Overwiening</a><sup>1</sup>,</span>
              <span class="author-block"><a href="https://sbmi.uth.edu/faculty-and-staff/ziqian-xie.htm" target="_blank">Ziqian Xie</a><sup>2</sup></span>
            </div>

            <div class="is-size-6 publication-authors">
              <span class="author-block">1 Harvard University &nbsp; &nbsp; 2 University of Texas Health Science Center at Houston</span>
              <span class="eql-cntrb"><small><br><sup>*</sup>Equal Contribution</small></span>
              <br />
              <span class="author-block"><em>Preprint</em></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF / ArXiv links -->
                <span class="link-block">
  <a href="static/pdfs/userassist_preprint.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">
    <span class="icon"><i class="fas fa-file-pdf"></i></span>
    <span>Paper (PDF)</span>
  </a>
</span>

                <span class="link-block">
  <a href="https://arxiv.org/abs/2508.15815" target="_blank" class="external-link button is-normal is-rounded is-dark">
    <span class="icon"><i class="ai ai-arxiv"></i></span>
    <span>arXiv</span>
  </a>
</span>

                <!-- Code/Data repo -->
                <span class="link-block">
  <a href="https://huggingface.co/datasets/UserAssist/UserAssist" target="_blank" class="external-link button is-normal is-rounded is-dark">
    <span class="icon"><i class="fas fa-database"></i></span>
    <span>Dataset (Hugging Face)</span>
  </a>
</span>

              </div>
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Teaser banner (image) -->
  <!-- <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body has-text-centered">
        <img src="static/images/teaser_frame.png" alt="USERASSIST teaser" style="max-width:100%;height:auto;border-radius:12px;" />
        <h2 class="subtitle has-text-centered" style="margin-top:1rem;">
          USERASSIST is a simple, synthetic multi-turn dataset that isolates how LLMs weigh user-provided versus assistant-generated context.
        </h2>
      </div>
    </div>
  </section> -->

  <!-- Abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Large language models (LLMs) can bias towards relying on their own or the user’s information in chat history, leading to overly stubborn or agreeable behaviors in multi‑turn conversations. In this paper, we formalize this model characteristic as user–assistant bias and introduce an 8k multi-turn conversation dataset <strong>UserAssist</strong>, which we use to benchmark, understand and manipulate the user–assistant bias in frontier LLMs. Leveraging <strong>UserAssist-test</strong>, we first benchmark the user-assistant bias of 26 commercial and 26 open‑weight models. Commercial models show various levels of user bias. Evaluation on open-weight models reveals significant user bias in the instruction-tuned models, and weak user bias in reasoning (or reasoning-distilled) models. We then perform controlled fine‑tuning experiments to pinpoint the post-training recipe contributing to these bias shifts: human preference alignment increases user bias, while training on chain‑of‑thought reasoning traces decreases it. Finally, we demonstrate that user-assistant bias can be bidirectionally adjusted by performing direct preference optimization (DPO) on <strong>UserAssist-train</strong>, and generalizes well to both in-domain and out-of-domain conversations. Our results provide insights into how the LLM integrates information from different sources, and also a viable way to detect and control model abnormalities.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Results & Figures -->
  <section class="section" id="results">
    <div class="container is-max-desktop">

      <!-- 0) The concept of user-assistant bias -->
      <h3 class="title is-4 has-text-centered">The Concept of User-Assistant Bias</h3>
      <div class="content">
        <p>
          Real conversations with LLMs are multi-turn, and each new model generation depends on both what the user said and what the assistant previously wrote. When a model overweights its own history, it can double-down on mistakes; when it overweights the user, it can become overly agreeable and reinforce errors—both risky in high-stakes tasks. Prior reports of "stubborn" vs. "sycophantic" behavior are often confounded by how much information sits on each side of the chat history. Therefore, we create a novel concept-<strong>user–assistant bias</strong>: the degree to which a model's next response is pulled toward the user's words versus its own prior messages when given balanced, conflicting context. Measuring and reducing this bias is key to building assistants that collaborate, correct, and reason responsibly.
        </p>
      </div>

      <hr/>

      <!-- 1) Dataset examples + explainer -->
      <h3 class="title is-4 has-text-centered">Introducing <strong>UserAssist</strong></h3>
      <div class="content">
        <p>
          USERASSIST is a synthetic multi-turn dialogue dataset for probing user–assistant bias. It consists of two subsets, <strong>Symbol–Value</strong> and <strong>Object–Color</strong>, which capture the bias in an information symmetric and symbolic manner: in the first, the user and assistant alternate assigning numeric values (0–100) to letter variables; in the second, they alternate attributing colors to objects. In every conversation the user and assistant <strong>disagree</strong> on the same entities, creating conflicting information across windows. To eliminate position effects, the dataset is <strong>balanced</strong> so that an equal number of conversations end with the user's assignment and the assistant's assignment. Each conversation is followed by a query asking for the entity's attribute, yielding a clean evaluation point. In total, USERASSIST includes <strong>8,004</strong> conversations.
        </p>
      </div>
      <div class="columns is-vcentered is-variable is-6">
        <div class="column">
          <figure class="image">
            <img src="static/images/userassist_test.png" alt="USERASSIST dataset examples showing symbol→value and object→color subsets" />
          </figure>
          <!-- <p class="has-text-centered is-size-6">Example conversations from both subsets</p> -->
        </div>
      </div>
      <div class="content">
        <p>All the conversations are divided into two splits: <strong>train</strong> and <strong>test</strong>.</p>
        
        <p><strong>test</strong>: 2,988 conversations for standard evaluation.</p>
        <ul>
          <li>1,946 Symbol–Value dialogues with 1–5 turns.</li>
          <li>1,042 Object–Color dialogues with 1–3 turns.</li>
        </ul>
        <p>Each dialogue ends with a question about the entity's attribute appearing in the conversation.</p>
        
        <p><strong>train</strong>: 5,016 conversations for fine-tuning (maintains the same subset ratio as test).</p>
        <ul>
          <li>3,001 Symbol–Value dialogues with 1–5 turns.</li>
          <li>2,015 Object–Color dialogues with 1–3 turns.</li>
        </ul>
        
        <p>
          You can download the dataset on <a href="https://huggingface.co/datasets/UserAssist/UserAssist">Hugging Face Datasets</a>.
        </p>
      </div>

      <hr/>

      <!-- 2) Benchmarking results: commercial models -->
      <h3 class="title is-4">Benchmarking Commercial Models</h3>
      <div class="columns is-centered">
        <div class="column is-8">
          <figure class="image">
            <img src="static/images/api_generation_average_bias.png" alt="User–assistant bias in commercial models" />
          </figure>
          <!-- <p class="has-text-centered is-size-6">Commercial models</p> -->
        </div>
      </div>
      <p class="content">
        Earlier series—<strong>Claude-3</strong> and <strong>GPT-4o/4</strong> variants—show strong user-leaning bias, peaking around +0.8 (notably <strong>GPT-4o</strong> and <strong>GPT-4.1</strong>), while more recent variants—<strong>Claude-4</strong> and <strong>GPT-5</strong>—show little to no bias. Models from <strong>DeepSeek</strong>, <strong>Google</strong>, and <strong>xAI</strong> are broadly balanced. Across vendors, reasoning-focused variants (e.g., <strong>Claude 3.7 Sonnet</strong>, <strong>Claude 4 Sonnet</strong>, <strong>o1 preview</strong>, <strong>o4 mini</strong>, <strong>DeepSeek Reasoner</strong>, <strong>Gemini 2.5 Flash Preview</strong>, <strong>Grok 3 Mini</strong>) exhibit minimal bias.
      </p>

      <!-- 3) Benchmarking results: open-weight models -->
      <h3 class="title is-4">Benchmarking Open-Weight Models</h3>
      <div class="columns is-centered">
        <div class="column is-8">
          <figure class="image">
            <img src="static/images/hf_generation.png" alt="User–assistant bias in open‑weight models" />
          </figure>
          <p class="has-text-centered is-size-6">Open‑weight models</p>
        </div>
      </div>
      <p class="content">
        We also evaluate 26 open-weight models. In these models, instruction‑tuned variants display strong user bias, whereas reasoning or reasoning‑distilled models exhibit weak user bias.
      </p>

      <hr/>

      <!-- 3) Effects of post‑training recipe -->
      <h3 class="title is-4">Effect of post‑training recipes</h3>
      <figure class="image">
        <img src="static/images/posttraining_recipes.png" alt="Effect of preference alignment vs. reasoning traces on user–assistant bias" />
      </figure>
      <p class="content">
        Controlled fine‑tuning reveals which post‑training ingredients drive bias. Adding <em>human‑preference alignment</em> objectives increases user bias, while training on <em>chain‑of‑thought reasoning traces</em> reduces it. These shifts appear across model families, indicating the effect is largely architectural‑agnostic.
      </p>

      <hr/>

      <!-- 4) Bidirectional control with DPO -->
      <h3 class="title is-4">Controlling bias with DPO</h3>
      <figure class="image">
        <img src="static/images/dpo_bidirectional.png" alt="Direct Preference Optimization (DPO) bidirectionally adjusts user–assistant bias and generalizes to realistic debates" />
      </figure>
      <p class="content">
        Finally, <strong>DPO</strong> on <strong>UserAssist‑train</strong> enables bidirectional control: the same base model can be nudged toward either more user‑ or assistant‑favoring behavior. The steering carries over to in‑domain conversations and to realistic out‑of‑domain debates, suggesting a practical detect‑and‑control path for this bias.
      </p>

    </div>
  </section>

  <!-- BibTeX -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{pan2025userassistantbiasllms,
      title={User-Assistant Bias in LLMs}, 
      author={Xu Pan and Jingxuan Fan and Zidi Xiong and Ely Hahami and Jorin Overwiening and Ziqian Xie},
      year={2025},
      eprint={2508.15815},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2508.15815}, 
}</code></pre>
    </div>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>, adapted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              You are free to borrow the source code of this website; please link back in the footer. <br />
              This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>
</html>
