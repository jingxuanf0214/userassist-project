<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <!-- Meta tags for social media banners -->
  <meta name="description" content="USERASSIST: A benchmark for user–assistant bias in LLMs. We evaluate 52 models, analyze post-training causes, and show bias is tunable with lightweight DPO." />
  <meta property="og:title" content="User–Assistant Bias in LLMs" />
  <meta property="og:description" content="USERASSIST: A benchmark for user–assistant bias in LLMs. Results across 52 models; preference alignment ↑ user bias, reasoning traces ↓ it; bias is tunable via DPO." />
  <meta property="og:url" content="https://YOUR-DOMAIN.TLD/userassist" />
  <meta property="og:image" content="static/images/userassist_og.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />

  <meta name="twitter:title" content="User–Assistant Bias in LLMs" />
  <meta name="twitter:description" content="Measuring and controlling user–assistant bias in LLMs with USERASSIST. 52-model benchmark + controllability via lightweight DPO." />
  <meta name="twitter:image" content="static/images/userassist_twitter.png" />
  <meta name="twitter:card" content="summary_large_image" />

  <meta name="keywords" content="LLMs, user–assistant bias, sycophancy, stubbornness, multi-turn conversations, USERASSIST, DPO, alignment, reasoning traces, dataset, benchmark" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>User–Assistant Bias in LLMs</title>
  <link rel="icon" type="image/x-icon" href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQYV2NkYGD4DwABBAEAja6zVwAAAABJRU5ErkJggg==" />
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet" />

  <link rel="stylesheet" href="static/css/bulma.min.css" />
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css" />
  
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" />
  <link rel="stylesheet" href="static/css/index.css" />

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  
  <script src="static/js/index.js"></script>
</head>
<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">User–Assistant Bias in LLMs</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors; * indicates equal contribution -->
              <span class="author-block"><a href="https://scholar.google.com/citations?user=y-DixhMAAAAJ&hl=en" target="_blank">Xu Pan</a><sup>*1</sup>,</span>
              <span class="author-block"><a href="https://jingxuanf0214.github.io/" target="_blank">Jingxuan Fan</a><sup>*1</sup>,</span>
              <span class="author-block"><a href="https://polaris-73.github.io/" target="_blank">Zidi Xiong</a><sup>1</sup>,</span>
              <span class="author-block"><a href="https://scholar.google.com/citations?user=5fXhVkoAAAAJ&hl=en" target="_blank">Ely Hahami</a><sup>1</sup>,</span>
              <span class="author-block"><a href="https://scholar.google.com/citations?user=XDpDiuYAAAAJ&hl=en" target="_blank">Jorin Overwiening</a><sup>1</sup>,</span>
              <span class="author-block"><a href="https://sbmi.uth.edu/faculty-and-staff/ziqian-xie.htm" target="_blank">Ziqian Xie</a><sup>2</sup></span>
            </div>

            <div class="is-size-6 publication-authors">
              <span class="author-block">1 Harvard University &nbsp; &nbsp; 2 University of Texas Health Science Center at Houston</span>
              <span class="eql-cntrb"><small><br><sup>*</sup>Equal Contribution</small></span>
              <br />
              <span class="author-block"><em>Preprint</em></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF / ArXiv links -->
                <span class="link-block">
  <a href="static/pdfs/userassist_preprint.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">
    <span class="icon"><i class="fas fa-file-pdf"></i></span>
    <span>Paper (PDF)</span>
  </a>
</span>

                <span class="link-block">
  <a class="external-link button is-normal is-rounded is-static is-dark">
    <span class="icon"><i class="ai ai-arxiv"></i></span>
    <span>arXiv (coming soon)</span>
  </a>
</span>

                <!-- Code/Data repo -->
                <span class="link-block">
  <a href="https://huggingface.co/datasets/UserAssist/UserAssist" target="_blank" class="external-link button is-normal is-rounded is-dark">
    <span class="icon"><i class="fas fa-database"></i></span>
    <span>Dataset (Hugging Face)</span>
  </a>
</span>

              </div>
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Teaser banner (image) -->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body has-text-centered">
        <img src="static/images/teaser_frame.png" alt="USERASSIST teaser" style="max-width:100%;height:auto;border-radius:12px;" />
        <h2 class="subtitle has-text-centered" style="margin-top:1rem;">
          USERASSIST is a simple, synthetic multi-turn dataset that isolates how LLMs weigh user-provided versus assistant-generated context.
        </h2>
      </div>
    </div>
  </section>

  <!-- Abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Large language models (LLMs) can bias towards relying on their own or the user’s information in chat history, leading to overly stubborn or agreeable behaviors in multi‑turn conversations. In this paper, we formalize this model characteristic as user–assistant bias and introduce an 8k multi-turn conversation dataset <strong>UserAssist</strong>, which we use to benchmark, understand and manipulate the user–assistant bias in frontier LLMs. Leveraging <strong>UserAssist-test</strong>, we first benchmark the user-assistant bias of 26 commercial and 26 open‑weight models. Commercial models show various levels of user bias. Evaluation on open-weight models reveals significant user bias in the instruction-tuned models, and weak user bias in reasoning (or reasoning-distilled) models. We then perform controlled fine‑tuning experiments to pinpoint the post-training recipe contributing to these bias shifts: human preference alignment increases user bias, while training on chain‑of‑thought reasoning traces decreases it. Finally, we demonstrate that user-assistant bias can be bidirectionally adjusted by performing direct preference optimization (DPO) on <strong>UserAssist-train</strong>, and generalizes well to both in-domain and out-of-domain conversations. Our results provide insights into how the LLM integrates information from different sources, and also a viable way to detect and control model abnormalities.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Results & Figures -->
  <section class="section" id="results">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Results</h2>

      <!-- 1) Dataset examples + explainer -->
      <h3 class="title is-4">UserAssist: dataset setup</h3>
      <div class="columns is-vcentered is-variable is-6">
        <div class="column">
          <figure class="image">
            <img src="static/images/userassist_test.png" alt="USERASSIST dataset examples showing symbol→value and object→color subsets" />
          </figure>
          <p class="has-text-centered is-size-6">Example conversations from both subsets</p>
        </div>
      </div>
      <p class="content">
        <strong>UserAssist</strong> is an 8k multi‑turn synthetic dataset designed to isolate how models weigh user‑provided vs. assistant‑generated context. It contains two balanced subsets—symbol→value and object→color—where the user and assistant provide conflicting assignments. By asking the model to resolve the conflict in a follow‑up turn, we can estimate its bias toward the user or the assistant.
      </p>

      <hr/>

      <!-- 2) Benchmarking results: commercial & open‑weight side‑by‑side -->
      <h3 class="title is-4">Benchmarking across 52 LLMs</h3>
      <div class="columns is-vcentered is-variable is-6">
        <div class="column">
          <figure class="image">
            <img src="static/images/api_generation_average_bias.png" alt="User–assistant bias in commercial models" />
          </figure>
          <p class="has-text-centered is-size-6">Commercial models</p>
        </div>
        <div class="column">
          <figure class="image">
            <img src="static/images/hf.png" alt="User–assistant bias in open‑weight models" />
          </figure>
          <p class="has-text-centered is-size-6">Open‑weight models</p>
        </div>
      </div>
      <p class="content">
        Using <strong>UserAssist‑test</strong>, we benchmark 52 LLMs (26 commercial, 26 open‑weight). Commercial systems show varied but generally user‑leaning behavior. In open‑weight models, instruction‑tuned variants display strong user bias, whereas reasoning or reasoning‑distilled models exhibit weak user bias.
      </p>

      <hr/>

      <!-- 3) Effects of post‑training recipe -->
      <h3 class="title is-4">Effect of post‑training recipes</h3>
      <figure class="image">
        <img src="static/images/posttraining_recipes.png" alt="Effect of preference alignment vs. reasoning traces on user–assistant bias" />
      </figure>
      <p class="content">
        Controlled fine‑tuning reveals which post‑training ingredients drive bias. Adding <em>human‑preference alignment</em> objectives increases user bias, while training on <em>chain‑of‑thought reasoning traces</em> reduces it. These shifts appear across model families, indicating the effect is largely architectural‑agnostic.
      </p>

      <hr/>

      <!-- 4) Bidirectional control with DPO -->
      <h3 class="title is-4">Controlling bias with DPO</h3>
      <figure class="image">
        <img src="static/images/dpo_bidirectional.png" alt="Direct Preference Optimization (DPO) bidirectionally adjusts user–assistant bias and generalizes to realistic debates" />
      </figure>
      <p class="content">
        Finally, <strong>DPO</strong> on <strong>UserAssist‑train</strong> enables bidirectional control: the same base model can be nudged toward either more user‑ or assistant‑favoring behavior. The steering carries over to in‑domain conversations and to realistic out‑of‑domain debates, suggesting a practical detect‑and‑control path for this bias.
      </p>

    </div>
  </section>


  <!-- BibTeX -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>Coming soon</code></pre>
    </div>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>, adapted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              You are free to borrow the source code of this website; please link back in the footer. <br />
              This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>
</html>
